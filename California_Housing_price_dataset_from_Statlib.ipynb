{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom pandas.plotting import scatter_matrix\nfrom sklearn.preprocessing import Imputer\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data_df = pd.read_csv(\"/kaggle/input/housing.csv\")\ndata_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df[\"ocean_proximity\"].value_counts() #categorical variable","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.info() #total_bedrooms has some null values to be handled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.describe() # shows that 255 housing_median_age are lower than 18, while 50% are lower than 29 and 75% are lower than 37","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.hist(bins=50, figsize=(20,15)) # shows histogram plots for only numerical attributes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def split_train_test(data, test_ratio):\n    np.random.seed(42)\n    shuffled_indices = np.random.permutation(len(data))\n    test_size = int(len(data)*test_ratio)\n    test_indices= shuffled_indices[:test_size]\n    train_indices= shuffled_indices[test_size:]\n    return data.iloc[train_indices], data.iloc[test_indices]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set, test_set = split_train_test(data_df, 0.2)\nprint(len(train_set), len(test_set))\n# or split by sklean api\nfrom sklearn.model_selection import train_test_split\ntrain_set, test_set = train_test_split(data_df, test_size=0.2, random_state=42)# this randomly selects instances/rows for training and test set\nprint(len(train_set), len(test_set))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\"\"\" since median_income seems to be an important attribute to decide median_house_value. So, we decide to divide the numerical values of this attribute \\\ninto catergoies/strata and we want to split our data such that we get equal instances from each category to avoid samping bias. This is called stratified sampling because we divide the data\\\nhomogeneously such that our data is representative of new cases that we may see in test_set and can generalize better\n\"\"\" \ndata_df[\"income_cat\"] = np.ceil(data_df[\"median_income\"]/1.5)\ndata_df[\"income_cat\"].value_counts() # we can see that most data is divided into 2.0 - 5.0.\ndata_df[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so we merge all of the data occuring latter it to one caegory i.e. 5.0\ndata_df[\"income_cat\"].where(data_df[\"income_cat\"]<5, 5.0, inplace=True)\ndata_df[\"income_cat\"].value_counts()\ndata_df[\"income_cat\"].hist()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Stratified Shuffle split of data based on income_cat\ndef stratified_spliting():\n    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n    for train_index, test_index in split.split(data_df, data_df[\"income_cat\"]):\n        strat_train_set = data_df.loc[train_index]\n        strat_test_set = data_df.loc[test_index]\n    return strat_train_set, strat_test_set","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set, strat_test_set = stratified_spliting()\nlen(strat_train_set), len(strat_test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set[\"income_cat\"].value_counts()/len(strat_train_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set[\"income_cat\"].value_counts()/len(strat_test_set) # now we can see that samples are evenly distributed among train and test set for income_cat column \\\n# whereas the test set generated by pure random sampling of train_test_split was skewed datasets","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for set in(strat_train_set, strat_test_set):\n    set.drop([\"income_cat\"], axis=1, inplace=True) #dropping the income_cat column from both sets ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_train_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"strat_test_set.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualize the data\n#visualizing geographical data for insight based on lattitude and logintude value\ndata_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,5))\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to see high density of data points more clearly , we set aplha = 0.1\ndata_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", figsize=(10,5), alpha=0.1)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_df.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.5, figsize=(20,15),\n             s= data_df[\"population\"]/100, label= \"population\", \n             c=\"median_house_value\", cmap = plt.get_cmap(\"jet\"), colorbar=True,) # s, the radius of the circle represents the population, while c, the color represents the prices\n# here blue color represents low price while red represents high price\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# so, the housing prices are very much related to the location(ex:- near to the ocean) and population_density\n# so, we will now use a clustering algoritm to detect main clusters and add new features that measure the proximity to the cluster centers\n#looking for correlations\ndata_df.drop([\"income_cat\"], axis=1, inplace=True)\ncorr_matrix = data_df.corr()\ncorr_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix[\"median_house_value\"].sort_values(ascending=False)\n# we notice a strong positive correlation between median_house_value and median_income\n# and a small negative correlation between median_house_value and longitude","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# to better visualize correlation of certain attributes, we are using pandas.plotting scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\", \"housing_median_age\"]\nscatter_matrix(data_df[attributes], figsize=(12,8))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# visualising median_house_value based on median_income\ndata_df.plot(kind=\"scatter\", x= \"median_income\", y= \"median_house_value\", figsize=(15,10))\n# the correlation is very strong, we can see the upward trend and the data is not too dispersed","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can see a straight line closely at 500000, another at 450000, another at 250000, another at 280000 and so on.\n# we could later remove thoe instances from our data before sending it to our machine learning algorithm for better prediction and to avoid quirks\n# As, we see strong positive correlation between households, population, total_rooms and total_bedrooms\ndata_df[\"rooms_per_household\"] = data_df[\"total_rooms\"]/data_df[\"households\"]\ndata_df[\"bedrooms_per_rooms\"] = data_df[\"total_bedrooms\"]/data_df[\"total_rooms\"]\ndata_df[\"population_per_household\"] = data_df[\"population\"]/data_df[\"households\"]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = data_df.corr()\ncorr_matrix[\"median_house_value\"].sort_values(ascending=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we see bedrooms_per_rooms is more strongly negatively correlated with median_house_value than total_bedrooms or total_rooms i.e. lower the no. of rooms , more price required\n# Also, rooms_per_household is more strongly related as compared to total_rooms or housholds\n# seperating predictors and labels from the strat_training_set\nhousing =  strat_train_set.drop(\"median_house_value\", axis=1)\nhousing_lables = strat_train_set[\"median_house_value\"].copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_lables.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#data cleaning\n#total_bedrooms has missing values. so we can either delete the attribute if its not that related for prediction or delete the instances with missing values or fill the missing values\n#delete the instances with missing values - housing.dropna(subset=[\"total_bedrooms\"])\n#delete the attribute - housing.drop[\"total_bedrooms\", axis = 1]\n#fill the missing values - median = housing[\"total_bedrooms\"].median()\n#hosuing[\"total_bedrooms\"].fillna(median)\n# save the median to fillna values in the test set too. and also for the new data.\n# or use imputer from skcit-learn, it fills na only for numerical attributes by calcluating their median\nimputer = Imputer(strategy=\"median\")\nhousing_num = housing.drop(\"ocean_proximity\", axis=1) # drop the categorical column for imputer\nimputer.fit(housing_num) # simply computes median of each attribute and stores it in statistics instance variable\nimputer.statistics_","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num.median().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X= imputer.transform(housing_num) # returns plain numpy array with transformed features\nhousing_tr = pd.DataFrame(X, columns=housing_num.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"housing_num.info()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}